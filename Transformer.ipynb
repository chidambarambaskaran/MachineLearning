{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN4Gsl548sStDwDIn/lDbxR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chidambarambaskaran/MachineLearning/blob/main/Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn\n",
        "import torch.optim as optim\n",
        "import math\n",
        "\n",
        "# Positional Encoding\n",
        "class PositionalEncoding(nn.Module):\n",
        "  def __init__(self, d_model, max_len = 5000):\n",
        "    super(PositionalEncoding, self).__init__()\n",
        "    pe = torch.zeros(max_len, d_model)\n",
        "    position = torch.arrange(0, max_len, dtype = torch.float).unsqueeze(1)\n",
        "    div_term = torch.exp(torch.arrange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
        "    pe[:, 0:2] = torch.sin(position * div_term)\n",
        "    pe[:, 1:2] = torch.cos(position * div_term)\n",
        "    self.pe = pe.unsqueeze(0)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return x + self.pe[:, :x.size(1), :].to(x.device)\n",
        "\n",
        "# Multi-Head Attention\n",
        "class MutliHeadAttention(nn.Module):\n",
        "  def __init__(self, d_model, num_heads):\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "    assert d_model % num_heads == 0\n",
        "    self.d_model = d_model\n",
        "    self.num_heads = num_heads\n",
        "    self.d_k = d_model // num_heads\n",
        "\n",
        "    self.q_linear = nn.Linear(d_model, d_model)\n",
        "    self.k_linear = nn.Linear(d_model, d_model)\n",
        "    self.v_Linear = nn.Linear(d_model, d_model)\n",
        "    self.out = nn.Linear(d_model, d_model)\n",
        "\n",
        "  def attention(self, Q, K, V, mask=None):\n",
        "    scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "    if mask is not None:\n",
        "      scores = scores.masked_fill(mask == 0, float('-inf'))\n",
        "    attn_weights = torch.nn.functional.softmax(scores, dim=-1)\n",
        "    return torch.matmul(scores, V)\n",
        "\n",
        "  def forward(self, q, k, v, mask=None):\n",
        "    batch_size = q.size(0)\n",
        "    Q = self.q_linear(q).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "    K = self.k_linear(k).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "    V = self.v_linear(v).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "    attn_output = self.attention(Q, K, V, mask)\n",
        "    attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
        "    return self.out(attn_output)\n",
        "\n",
        "# FeedForward\n",
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, d_model, d_ff):\n",
        "    super(FeedForward, self).__init__()\n",
        "    self.fc1 = nn.Linear(d_model, d_ff)\n",
        "    self.fc2 = nn.Linear(d_model, d_ff)\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.fc2(self.relu(self.fc1(x)))\n",
        "\n",
        "# Encoder Layer\n",
        "class EncoderLayer(nn.Module):\n",
        "  def __init__(self, d_model, max_len, d_ff):\n",
        "    super(EncoderLayer, self).__init__()\n",
        "    self.self.attn = MultiHeadAttention(d_model, num_heads)\n",
        "    self.feed_forward = FeedForward(d_model, d_ff)\n",
        "    self.norm1 = nn.LayerNorm(d_model)\n",
        "    self.norm2 = nn.LayerNorm(d_model)\n",
        "\n",
        "  def forward(self, x, mask=None):\n",
        "    attn_out = self.self_attn(x, x, x, mask)\n",
        "    x = self.norm1(x + attn_out)\n",
        "    ff_out = self.feed_forward(x)\n",
        "    return self.norm2(x + ff_out)\n",
        "\n",
        "# Full Encoder\n",
        "class Encoder(nn.Module):\n",
        "  def __init__(self, vocab_size, d_model, max_len, d_ff, num_layers, num_heads):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "    self.positional_encoding = PositionalEncoding(d_model, max_len)\n",
        "    self.layers = nn.ModuleList([EncoderLayer(d_model, d_ff, num_heads)for _ in range(num_layers)])\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.embedding(x)\n",
        "    x = self.positional_encoding(x)\n",
        "    for layer in self.layers:\n",
        "      x = layer(x, mask)\n",
        "    return x\n",
        "\n",
        "# Decoder Layer\n",
        "class DecoderLayer(nn.Module):\n",
        "  def __init__(self, d_model, num_heads, d_ff):\n",
        "    super(DecoderLayer. self).__init__()\n",
        "    self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "    self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
        "    self.feed_forward = FeedForward(d_model, d_ff)\n",
        "    self.norm1 = nn.LayerNorm(d_model)\n",
        "    self.norm2 = nn.LayerNorm(d_model)\n",
        "    self.norm3 = nn.LayerNorm(d_model)\n",
        "\n",
        "  def forward(self, x, enc_output, src_mask=None, tgt_mask=None):\n",
        "    attn_out = self.self_attn(x, x, x, tgt_mask)\n",
        "    x = self.norm1(x + attn_out)\n",
        "    attn_out = self.cross_attn(x, enc_output, enc_output, src_mask)\n",
        "    x = self.norm2(x + attn_out)\n",
        "    ff_out = self.feed_forward(x)\n",
        "    return self.norm3(ff_out + x)\n",
        "\n",
        "# Full Decoder\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, num_heads, d_ff, num_layers, max_len):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoding = PositionalEncoding(d_model, max_len)\n",
        "        self.layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff) for _ in range(num_layers)])\n",
        "\n",
        "    def forward(self, x, enc_output, src_mask=None, tgt_mask=None):\n",
        "        x = self.embedding(x)\n",
        "        x = self.pos_encoding(x)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, enc_output, src_mask, tgt_mask)\n",
        "        return x\n",
        "\n",
        "# Transformer\n",
        "class Transformer(nn.Module):\n",
        "  def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, d_ff, num_layers, max_len):\n",
        "    super(Transformer, self).__init__()\n",
        "    self.encoder = Encoder(src_vocab_size, d_model, num_heads, d_ff, num_layers, max_len)\n",
        "    self.decoder = Decoder(tgt_vocab_size, d_model, num_heads, d_ff, num_layers, max_len)\n",
        "    self.final_layer = nn.Linear(d_model, tgt_vocab_size)\n",
        "\n",
        "  def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n",
        "    enc_output = self.encoder(src, src_mask)\n",
        "    dec_ouput = self.decoder(tgt, enc_ouput, src_mask, tgt_mask)\n",
        "    return self.final_layer(dec_output)\n",
        "\n",
        "# Hyperparameters\n",
        "src_vocab_size = tgt_vocab_size = 10000\n",
        "d_model = 512\n",
        "num_heads = 8\n",
        "d_ff = 2048\n",
        "num_layers = 6\n",
        "max_len = 100\n",
        "\n",
        "# Initialize Transformer\n",
        "model = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_heads, d_ff, num_layers, max_len)\n"
      ],
      "metadata": {
        "id": "Gf6AiHllCkkN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}